<div align="center">

# Monthly Baseflow Dataset for the CONUS

provides an opportunity to analyze large-scale baseflow trends under global change üî•<br>

</div>
<br>

<div align="center">

![](https://user-images.githubusercontent.com/29588684/142756866-7e22814d-2e78-4fad-8035-86eee529bb10.gif)

</div>
<br>

- [Introduction](#introduction)
- [Project Structure](#project-structure)
- [Usage](#usage)
    - [Data Preparation](#data-preparation)
    - [Hyperparameter Tuning](#hyperparameter-tuning)
    - [Train and Evaluate](#train-and-evaluate)
    - [Baseflow Simulation](#baseflow-simulation)
- [License](#license)
<br>

## üìå&nbsp;&nbsp;Introduction
To fill the gaps in time-varying baseflow datasets, we introduced a machine learning approach called the long short-term memory (LSTM) network to develop a monthly baseflow dataset.

To better train across basins, we compared the standard LSTM with four variant architectures using additional static properties as input. Results show that three variant architectures (Joint, Front, and EA-LSTM) perform better than the standard LSTM, with median Kling-Gupta efficiency across basins greater than 0.85.

Based on Front LSTM, the monthly baseflow dataset with 0.25¬∞ spatial resolution across the contiguous United States from 1981 to 2020 was obtained, which can be downloaded from the [release page](https://github.com/xiejx5/BaseFlowCONUS/releases).
<br>
<br>

## ‚ö°&nbsp;&nbsp;Project Structure
```yaml
‚îú‚îÄ‚îÄ configs                 <- Hydra configuration files
‚îÇ   ‚îú‚îÄ‚îÄ constant                <- Folder paths and constants
‚îÇ   ‚îú‚îÄ‚îÄ dataset                 <- Configs of Pytorch dataset
‚îÇ   ‚îú‚îÄ‚îÄ datasplit               <- Split dataset into train and test
‚îÇ   ‚îú‚îÄ‚îÄ hydra                   <- Configs of Hydra logging and launcher
‚îÇ   ‚îú‚îÄ‚îÄ loss                    <- Configs of loss function
‚îÇ   ‚îú‚îÄ‚îÄ model                   <- Configs of Pytorch model architectures
‚îÇ   ‚îú‚îÄ‚îÄ optimizer               <- Configs of optimizer
‚îÇ   ‚îú‚îÄ‚îÄ trainer                 <- Configs of validation metrics and trainer
‚îÇ   ‚îú‚îÄ‚îÄ tuner                   <- Configs of Optuna hyperparameter search
‚îÇ   ‚îî‚îÄ‚îÄ config.yaml             <- Main project configuration file
‚îÇ
‚îú‚îÄ‚îÄ data                    <- Baseflow, time series, and static properties
‚îÇ
‚îú‚îÄ‚îÄ logs                    <- Logs generated by Hydra and PyTorch loggers
‚îÇ
‚îú‚îÄ‚îÄ saved                   <- Saved evaluation results and model parameters
‚îÇ
‚îú‚îÄ‚îÄ src
‚îÇ   ‚îú‚îÄ‚îÄ datasets                <- PyTorch datasets
‚îÇ   ‚îú‚îÄ‚îÄ datasplits              <- Dataset splitter for train and test
‚îÇ   ‚îú‚îÄ‚îÄ models                  <- PyTorch model architectures
‚îÇ   ‚îú‚îÄ‚îÄ trainer                 <- Class managing training process
‚îÇ   ‚îú‚îÄ‚îÄ utils                   <- Utility scripts for metric logging
‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py             <- Model evaluation piplines
‚îÇ   ‚îú‚îÄ‚îÄ perpare.py              <- Data preparation piplines
‚îÇ   ‚îî‚îÄ‚îÄ simulate.py             <- Simulate gridded baseflow
‚îÇ
‚îú‚îÄ‚îÄ run.py                  <- Run pipeline with chosen configuration
‚îÇ
‚îú‚îÄ‚îÄ main.py                 <- Main process for the whole project
‚îÇ
‚îú‚îÄ‚îÄ .gitignore              <- List of files/folders ignored by git
‚îú‚îÄ‚îÄ requirements.txt        <- File for installing python dependencies
‚îú‚îÄ‚îÄ LICENSE
‚îî‚îÄ‚îÄ README.md
```
<br>

<a name="usage"></a>
## ‚ÑπÔ∏è&nbsp;&nbsp;Usage

### Data Preparation
```bash
from src import prepare

# download data from ERA5 and Google Earth Engine
prepare(cfg['constant'])
```

### Hyperparameter Tuning
```bash
# detailed settings are in optuna.yaml
python run.py -m tuner=optuna
```

### Train and Evaluate
```bash
# evaluate Front LSTM using test_size=0.2
python run.py -m model=front dataset.eco=CPL, NAP, NPL
```

```bash
# train Front LSTM using test_size=0
python run.py -m model=front datasplit=full dataset.eco=CPL, NAP, NPL
```

### Baseflow Simulation
```bash
from src import simulate

# load the trained model for each ecoregion
checkpoint = 'saved/train/front/CPL/models/model_latest.pth'
simulate(checkpoint)
```
<br>

## üöÄ&nbsp;&nbsp;License
