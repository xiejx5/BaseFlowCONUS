<div align="center">

# Monthly Baseflow Dataset for the CONUS

</div>
<br>

<div align="center">

![](https://user-images.githubusercontent.com/29588684/142756866-7e22814d-2e78-4fad-8035-86eee529bb10.gif)

</div>
<br>

- [Introduction](#introduction)
- [Folder Structure](#project-structure)
- [Usage](#introduction)
    - [Data Preparation](#data-preparation)
    - [Hyperparameter Tuning](#hyperparameter-tuning)
    - [Train and Evaluate](#train-and-evaluate)
    - [Baseflow Simulation](#baseflow-simulation)
- [Citation](#citation)
<br>


## ğŸ“Œ&nbsp;&nbsp;Introduction
To fill the gaps in time-varying baseflow datasets, we introduced a machine learning approach called the long short-term memory (LSTM) network to develop a monthly baseflow dataset.

To better train across basins, we compared the standard LSTM with four variant architectures using additional static properties as input. Results show that three variant architectures (Joint, Front, and EA-LSTM) perform better than the standard LSTM, with median Kling-Gupta efficiency across basins greater than 0.85.

Based on Front LSTM, the monthly baseflow dataset with 0.25Â° spatial resolution across the contiguous United States from 1981 to 2020 was obtained, which can be downloade from the [release page](https://github.com/xiejx5/BaseFlowCONUS/releases).
<br>
<br>

## âš¡&nbsp;&nbsp;Project Structure
```yaml
baseflow/
â”œâ”€â”€ configs                 <- Hydra configuration files
â”‚   â”œâ”€â”€ constant                <- Folder paths and constants
â”‚   â”œâ”€â”€ dataset                 <- Configs of Pytorch dataset
â”‚   â”œâ”€â”€ datasplit               <- Split dataset into train and test
â”‚   â”œâ”€â”€ hydra                   <- Configs of Hydra logging and launcher
â”‚   â”œâ”€â”€ loss                    <- Configs of loss function
â”‚   â”œâ”€â”€ model                   <- Configs of Pytorch model architectures
â”‚   â”œâ”€â”€ optimizer               <- Configs of optimizer
â”‚   â”œâ”€â”€ trainer                 <- Configs of validation metrics and trainer
â”‚   â”œâ”€â”€ tuner                   <- Configs of Optuna hyperparameter search
â”‚   â””â”€â”€ config.yaml             <- Main project configuration file
â”‚
â”œâ”€â”€ data                    <- Baseflow, time series, and static properties
â”‚
â”œâ”€â”€ logs                    <- Logs generated by Hydra and PyTorch loggers
â”‚
â”œâ”€â”€ saved                   <- Saved evaluation results and model parameters
â”‚
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ datasets                <- PyTorch datasets
â”‚   â”œâ”€â”€ datasplits              <- Dataset splitter for train and test
â”‚   â”œâ”€â”€ models                  <- PyTorch model architectures
â”‚   â”œâ”€â”€ trainer                 <- Class managing training process
â”‚   â”œâ”€â”€ utils                   <- Utility scripts for metric logging
â”‚   â”œâ”€â”€ evaluate.py             <- Model evaluation piplines
â”‚   â”œâ”€â”€ perpare.py              <- Data preparation piplines
â”‚   â””â”€â”€ simulate.py             <- Simulate gridded baseflow
â”‚
â”œâ”€â”€ run.py                  <- Run pipeline with chosen configuration
â”‚
â”œâ”€â”€ main.py                 <- Main process for the whole project
â”‚
â”œâ”€â”€ .gitignore              <- List of files/folders ignored by git
â”œâ”€â”€ requirements.txt        <- File for installing python dependencies
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```
<br>

## â„¹ï¸&nbsp;&nbsp;Usage

### Data Preparation
<br>

### Hyperparameter Tuning
<br>

### Train and Evaluate
<br>

### Baseflow Simulation
<br>

## ğŸš€&nbsp;&nbsp;Citation

