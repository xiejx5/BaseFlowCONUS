# @package _global_

# example hyperparameter optimization of some experiment with Optuna:
# python run.py -m tuner=optuna hydra.sweeper.n_trials=30

defaults:
    - override /model: rear
    - override /dataset: with_static
    - override /datasplit: cross_val
    - override /hydra/sweeper: optuna
    # The value of sampler must be one of tpe, random, cmaes, nsgaii and motpe
    - override /hydra/sweeper/sampler: random
    - override /hydra/launcher: joblib
    - override /hydra/job_logging: disabled
    - _self_

# use monitor to choose metric which will be optimized by Optuna
trainer:
    monitor: max MedianNSE/valid
    early_stop: 50

hydra:
    launcher:
        n_jobs: ${hydra.sweeper.n_jobs}

    # here we define Optuna hyperparameter search
    # it optimizes for value returned from function with @hydra.main decorator
    # learn more here: https://hydra.cc/docs/next/plugins/optuna_sweeper
    sweeper:
        storage: null
        study_name: ${dataset.eco}
        n_jobs: 5

        # 'minimize' or 'maximize' the objective
        direction: maximize

        # number of experiments that will be executed
        n_trials: 200

        # choose Optuna hyperparameter sampler
        # learn more here: https://optuna.readthedocs.io/en/stable/reference/samplers.html
        sampler:
            _target_: optuna.samplers.RandomSampler
            seed: 12345

        # define range of hyperparameters
        params:
            optimizer.lr: range(0.0001, 0.0009, step=0.0004)
            model.hidden_size: choice(64, 128, 256, 512)
            model.rear_size: choice(16, 32, 64, 128, 256, 512)
            model.drop_prob: choice(0, 0.25, 0.4, 0.5)
            dataset.seq_length: range(6, 24, step=6)
